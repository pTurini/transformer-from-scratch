{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding GPT from scratch:\n",
    "(Only using pyTorch for neural network training and encoding)\n",
    "\n",
    "This projects layouts the base structure and architecture for a language model, skipping the training and finetuning on large data (for obvious purposes). This is a character based LLM, rather than a whole word one, for simplicity purposes.\n",
    "\n",
    "Its main purpose is to learn and consolidate my knowledge of the transformer architecture. I plan to follow it up, building the neural network from scratch (which I have already done in C), and then explore more advanced topics of LLMs, such as RAG, CAG, LPMs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted. \n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# Extracts the Tiny-Shakespeare and stores it in a variable:\n",
    "url =\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "r = requests.get(url)\n",
    "if r.status_code == 200:\n",
    "    text = r.text\n",
    "    print(\"Dataset extracted. \\n\")\n",
    "else:\n",
    "    print(f\"Failed to fetch. Status: {r.status_code}\")\n",
    "\n",
    "print(text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      " \n",
      " Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "#Organizes the vocabulary (in this case unique characters, rather than words)\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\" \\n Vocabulary size: \"+ str(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 53, 40, 40, 47, 58, 57]\n",
      "Hobbits\n"
     ]
    }
   ],
   "source": [
    "#Tokenizing the input text, encoding individual characters into vectors or integers\n",
    "string_to_token = { ch: i for i, ch in enumerate(chars)} #creates dictionary with character as key and token as value\n",
    "token_to_string = {i: ch for i, ch in enumerate(chars)} #creates dictionary with token as key and character as value\n",
    "\n",
    "encode = lambda s : [string_to_token[c] for c in s] #given a string, give the corresponding series of tokens\n",
    "decode = lambda l : ''.join([token_to_string[i] for i in l])#given a series of tokens, return the strings (concats into a single string)\n",
    "\n",
    "print(encode(\"Hobbits\"))\n",
    "print(decode(encode(\"Hobbits\")))\n",
    "#Google uses Sentrencepeice for encoding, which encodes subwords.\n",
    "#OpenAI uses tiktoken, which is a BPE tokenizer, with around 50k tokens.\n",
    "#There is a trade-off between vocabulary size and token size. Thus subword is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1115394]), torch.int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenizing the whole tinyshakespeare dataset, using pyTorch:\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype = torch.long) # the data tensor is tikenized in its entirety\n",
    "data.shape, data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data[:200]) #tokenized the full dataset into a sequence of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and validation sets, 90% for training\n",
    "n = int(0.9*len(data)) #casts 0.9*len(data) to int\n",
    "train_data = data[:n] #indexes 90% as training\n",
    "val_data = data[n:] #the rest is validation, checking for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feeding the text into the transformer by chunking the dataset and then sample it for training\n",
    "block_size = 8 #AKA context window size\n",
    "train_data[:block_size+1] #first context window of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([18]), the target is: 47\n",
      "When input is tensor([18, 47]), the target is: 56\n",
      "When input is tensor([18, 47, 56]), the target is: 57\n",
      "When input is tensor([18, 47, 56, 57]), the target is: 58\n",
      "When input is tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "When input is tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "#the training is done by guessing the next word, but for every single subsequence as well, in parallel\n",
    "#the chunks of 9 characters actually only contain 8 examples, since the first word is the seed and must be present.\n",
    "x = train_data[:block_size] #1st window, context\n",
    "y = train_data[1:block_size+1] #2nd sliding window, it is the target\n",
    "for t in range(block_size):\n",
    "    context= x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context}, the target is: {target}\")\n",
    "\n",
    "#The training is done from context 1 to block_size in parallel. It makes it more efficient to train. \n",
    "#And allows the transformer to percieve long and short term relationships between words, form 1 to block_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "==============\n",
      "When input is [24], the target is: 43\n",
      "When input is [24, 43], the target is: 58\n",
      "When input is [24, 43, 58], the target is: 5\n",
      "When input is [24, 43, 58, 5], the target is: 57\n",
      "When input is [24, 43, 58, 5, 57], the target is: 1\n",
      "When input is [24, 43, 58, 5, 57, 1], the target is: 46\n",
      "When input is [24, 43, 58, 5, 57, 1, 46], the target is: 43\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target is: 39\n",
      "When input is [44], the target is: 53\n",
      "When input is [44, 53], the target is: 56\n",
      "When input is [44, 53, 56], the target is: 1\n",
      "When input is [44, 53, 56, 1], the target is: 58\n",
      "When input is [44, 53, 56, 1, 58], the target is: 46\n",
      "When input is [44, 53, 56, 1, 58, 46], the target is: 39\n",
      "When input is [44, 53, 56, 1, 58, 46, 39], the target is: 58\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target is: 1\n",
      "When input is [52], the target is: 58\n",
      "When input is [52, 58], the target is: 1\n",
      "When input is [52, 58, 1], the target is: 58\n",
      "When input is [52, 58, 1, 58], the target is: 46\n",
      "When input is [52, 58, 1, 58, 46], the target is: 39\n",
      "When input is [52, 58, 1, 58, 46, 39], the target is: 58\n",
      "When input is [52, 58, 1, 58, 46, 39, 58], the target is: 1\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target is: 46\n",
      "When input is [25], the target is: 17\n",
      "When input is [25, 17], the target is: 27\n",
      "When input is [25, 17, 27], the target is: 10\n",
      "When input is [25, 17, 27, 10], the target is: 0\n",
      "When input is [25, 17, 27, 10, 0], the target is: 21\n",
      "When input is [25, 17, 27, 10, 0, 21], the target is: 1\n",
      "When input is [25, 17, 27, 10, 0, 21, 1], the target is: 54\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target is: 39\n"
     ]
    }
   ],
   "source": [
    "#multiple blocks are computed in parallel into the transformer model, taking advantage of the GPUs.\n",
    "#this batching is done for efficiency, and they are processed independently\n",
    "\n",
    "torch.manual_seed(1337) #makes RNG stable for control and debugging purposes\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    #generates the batch with inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #generates batch_size random positions between 0 and the total indexable training set\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y= torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('Inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('Targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print(\"==============\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"When input is {context.tolist()}, the target is: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#feeding it into a simple language model, the Bigram Language Model:\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337) #makes RNG stable for control and debugging purposes\n",
    "\n",
    "class BigramLanguageModel(nn.Module): #subclass of nn module\n",
    "\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # creates the embedding table\n",
    "    def forward(self, idx, targets = None):\n",
    "        logits = self.token_embedding_table(idx) #takes the input and looks it up in the embedding table. \n",
    "        # 24 takes the 24th row from the embedding table\n",
    "        # PyTorch arranges it into a Batch, Time, Channel tensor: (B=batch_size=4, T=block_size=8, C=vocab_size=65)\n",
    "        if targets is None:\n",
    "            loss =None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) #reshapes logit vector\n",
    "            targets = targets.view(B*T) #reshapes target vector\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #get predictions\n",
    "            logits, loss = self(idx)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:,-1,:] #(B,C)\n",
    "            #apply softmax\n",
    "            probs = F.softmax(logits, dim =-1) # (B,C)\n",
    "            # sample from distribution randomly, generating prediction for each batch\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            #append sampled index to running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss) #Since we have 65 distinct vocab items, the expected loss is -log(1/65) = 4.174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    " #creates a seed token as 0\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens=100)[0].tolist())) #creates 100 character/token generation, from the 0th batch\n",
    "#it makes no sense, as it hasn't been trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the bigram model:\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4070'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0) #checking if my GPU is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5727508068084717\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range (10000):\n",
    "    #sample new batch of data\n",
    "    xb,yb = get_batch('train')\n",
    "\n",
    "    #calculate loss\n",
    "    logits, loss = m(xb,yb) #evaluate the loss\n",
    "    optimizer.zero_grad(set_to_none=True) #zeroing \n",
    "    loss.backward() #getting the gradients through backprop\n",
    "    optimizer.step() #updates parameters\n",
    "\n",
    "print(loss.item()) #loss is improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
      "KIN d pe wither vouprrouthercc.\n",
      "hathe; d!\n",
      "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
      "h hay.JUCle n prids, r loncave w hollular s O:\n",
      "HIs; ht anjx?\n",
      "\n",
      "DUThinqunt.\n",
      "\n",
      "LaZAnde.\n",
      "athave l.\n",
      "KEONH:\n",
      "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n",
      "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
      "I hisgothers je are!-e!\n",
      "QLYotouciullle'z\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype = torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating self-attention\n",
    "Self-attention allows for embeddings to change meaning/share information based on context, potentially over a large distance in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mathematical trick, toy example\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 8 tokens in the batch are uncoupled, and do not communicate. Tokens should not communicate with future tokens.\n",
    "\n",
    "So tokens at $T_i$ should only interact with tokens $T_{i-1}$ and so on. Information flows from previous context only.\n",
    "\n",
    "One (weak) way of doing this is through the average of preceding tokens. It loses spatial information, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every token, calculate the average of all previous tokens (itself included)\n",
    "#x[b,t] = mean_{i<=t} x [b,i]\n",
    "xbow = torch.zeros((B,T,C))#BAG OF WORDS\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev,0) #averages time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1808, -0.0700],\n",
       "         [-0.3596, -0.9152],\n",
       "         [ 0.6258,  0.0255],\n",
       "         [ 0.9545,  0.0643],\n",
       "         [ 0.3612,  1.1679],\n",
       "         [-1.3499, -0.5102],\n",
       "         [ 0.2360, -0.2398],\n",
       "         [-0.9211,  1.5433]]),\n",
       " tensor([[ 0.1808, -0.0700],\n",
       "         [-0.0894, -0.4926],\n",
       "         [ 0.1490, -0.3199],\n",
       "         [ 0.3504, -0.2238],\n",
       "         [ 0.3525,  0.0545],\n",
       "         [ 0.0688, -0.0396],\n",
       "         [ 0.0927, -0.0682],\n",
       "         [-0.0341,  0.1332]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0], xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3)) #returns the lower triangular portion of the ones matrix\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= \n",
      "\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b= \n",
      "\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c= \n",
      "\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "#Matrix multiplication allows for optimization\n",
    "torch.manual_seed(42)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b \n",
    "print('a= \\n')\n",
    "print(a)\n",
    "print('b= \\n')\n",
    "print(b)\n",
    "print('c= \\n')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= \n",
      "\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b= \n",
      "\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c= \n",
      "\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "#this allows cumulative sums of the lines in matrix a in steps\n",
    "#we are essentialy doing the sum of a variable number of rows in a. \n",
    "#average can be done in incremental fashion by normalizing each row\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a= a/torch.sum(a,dim=1, keepdim=True)\n",
    "c = a @ b \n",
    "print('a= \\n')\n",
    "print(a)\n",
    "print('b= \\n')\n",
    "print(b)\n",
    "print('c= \\n')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#making the averaging code more efficient:\n",
    "wei = torch.tril(torch.ones(T,T)) #weights\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow2 = wei @ x #(BxTxT) @ (BxTxC) --> (BxTxC)\n",
    "torch.allclose(xbow,xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]]),\n",
       " tensor([[ 1.3488, -0.1396],\n",
       "         [ 0.8173,  0.4127],\n",
       "         [-0.1342,  0.4395],\n",
       "         [ 0.2711,  0.4774],\n",
       "         [ 0.2421,  0.0694],\n",
       "         [ 0.0084,  0.0020],\n",
       "         [ 0.0712, -0.1128],\n",
       "         [ 0.2527,  0.2149]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[1],xbow2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v3: softmax\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) #tokens from the future  cannot communicate\n",
    "wei = F.softmax(wei, dim = -1) #goes through softmax to generate a pdf\n",
    "xbow3 = wei @ x #this is the step that actually updates the embeddings (aggregation)\n",
    "torch.allclose(xbow2, xbow3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei #softmax of the masked dist with zeroes as -inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much of each token from the past must we incorporate? It depends on the wei matrix, of course.\n",
    "\n",
    "But different words have different interactions between themselves. Tokens cannot communicate to tokens from the future.\n",
    "\n",
    "So essentially, weighted aggregations from past context/elements can be done with a matrix multiplication with lower triangular fashion. The elements in the lower part communicate how much meaning or interaction they must communicate or fuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self-attention:\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4, 8, 32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "#single head\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C,head_size, bias=False)\n",
    "value = nn.Linear(C,head_size, bias=False)\n",
    "k = key(x) # (B,T,16)\n",
    "q = query(x) # (B,T,16)\n",
    "#communication:\n",
    "\n",
    "wei = q @ k.transpose(-2,-1) # (B, T, 16) @ (B, 16, T) == (B,T,T)\n",
    "tril= torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei,dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weights obviously change between letters/tokens -> gather information through the past in a data-depend weight\n",
    "\n",
    "Every single token in the array will emit a query and a key vector. Query: what I'm looking for. Key: what I contain.\n",
    "The attention pattern times the values matrix is the new weight matrix, constructed by the dot product between all keys and queries.\n",
    "\n",
    "Attention can be applied to any directed graph. There is no notion to space, this is why the tokens are positionally encoded.\n",
    "\n",
    "The batches do not communicate.\n",
    "\n",
    "The future tokens do not communicate to past tokens in LLMs. Sentiment analysis is one example of fully connected tokens (because it is not predictive), so a self-attention ENCODER block is used.\n",
    "\n",
    "For LLMs, the previously described construction is a DECODER block, with the -inf mask, so that the future values are not available to the past.\n",
    "\n",
    "Self-attention has keys, queries and values coming from the same source, x. Cross-attention takes key, queries and values from different sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Attention(Q,K,V) = softmax(\\frac{Q\\cdot K^T}{\\sqrt{d_k}}) \\cdot V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is scaled by $\\sqrt{d_k}$ because the variance of the weight matrix would be in the order of the head size $d_k$.\n",
    "If attention is normalized, the variance will be 1. Wei will be fed into softmax, so it must be fairly diffuse to not saturate (they will converge to one-hot vectors otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9400), tensor(1.0119), tensor(15.1440))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1) \n",
    "k.var(), q.var(), wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0172), tensor(0.9599), tensor(0.8116))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2,-1) * head_size**-0.5\n",
    "k.var(), q.var(), wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5]),dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5851e-02, 7.8918e-04, 1.1713e-01, 7.8918e-04, 8.6545e-01])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1,-0.2,0.3,-0.2,0.5])*10,dim=-1) #sharpens the pdf towards the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head of attention module\n",
    "n_embd =8\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        #compute attention\n",
    "        wei = q @ k.transpose(-2,-1) *C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim =-1)\n",
    "        v= self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "    \n",
    "# this is to be instantiated before the lm head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing multi-head attention: applying multiple attention layers in parallel and sum results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
